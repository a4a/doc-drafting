\documentclass[a4paper,english]{article}
\usepackage{a4a}

\begin{document}

\section*{Coverpage}

% set up R environment
<<echo=FALSE, results="hide">>=
library(RColorBrewer)
library(lattice)
library(xtable)
@

<<echo=FALSE>>=
cat("Document build date:", date(), "\n")
cat("Working directory :\n", "    ", getwd(), "\n")
cat("Current contents of .GlobalEnv:\n\n")
cat("    ", if (length(ls(all = TRUE)) == 0) "<empty>" else ls(all = TRUE))
cat("\nSession information:\n\n")
sessionInfo()
@


\title{Description of Catch at Age model}
\author{Colin P. Millar, Ernesto Jardim}
\date{}
\maketitle

\section{Motivation}

The goal is to describe an age based model that is robust and easy to use.  Robustness here means robust parameter estimation in addition to being robust to (underlying) model complexity.  We aim to provide robust parameter estimates by concentrating parameters out of the objective function where possible and using structured random effects to allow for complexity when the data support it.  More on this in the coming sections, but first a brief description of the data that the model will have to fit to.


\section{Data and model}

The data are
\begin{itemize}
  \item[$C_{at}$] catch at age $a$ and year $t$
  \item[$S_{atk}$] abundance index for age $a$ and year $t$ from the $k$th survey or CPUE series, $k = 1, 2, \ldots$
\end{itemize}

The model is an age structure model where the number of fish in a given cohort $N$ at the start of the following year is the number of fish that survived the perils of the current year.  We assume that fish die through the year at a constant rate $e^{-Z}$ ($Z$ is positive), and that this rate is solely due to natural causes ($M$) and fishing ($F$) so that the total mortality rate is $Z = F + M$.  This results in the model
\begin{align*}
  N_{a+1,t+1} = N_{at} e^{- Z_{at}}
\end{align*}
Abundance indices are observations of the relative abundance not of absolute abundance.  This is because trawl surveys do not detect every fish but a fixed proportion $Q$. This proportion depends on age through length and means the index is proportional to abundance
\begin{align*}
  S_{at} &= Q_a N_{at}
\end{align*}
If F and M are constant through the year catches arise as a fraction of those fish that died, and is written here as the familiar Baranov catch equation
\begin{align*}
  C_{at} &= \frac{F_{at}}{Z_{at}}\left(N_{at} - N_{a+1,t+1} \right) \\
         &= \frac{F_{at}}{Z_{at}}\left(1 - e^{- Z_{at}}\right) N_{at}
\end{align*}
These last two equations show that in there own way, catches and abundance indices are both observations of the numbers of fish in the population.  Neither is sufficient to estimate the absolute abundances $N$ but together they can be used to estimate both $N$ and $F$.  One way of doing this is using a statistical catch at age approach  \citep[see for example][]{Myers_Cadigan.94} where much like in a regression model we find the values of $F$ and $N$ that give the best fit to our observations.  The only specification we need to make is how our observations come about, or how are observations distributed about the model predictions.  We assume that catches and indices are normally distributed on the log scale.  This is for two reasons: 1) catches and indices are typically positive with variances that increase as the level increases in such a way that taking logs makes the variance constant. 2) the equations given above become easier to deal with on the log scale as we will show later.  This leads to the following distributional assumptions
\begin{align*}
 c_{at} &\sim N \Bigg( \log \left( \frac{F_{at}}{Z_{at}} \left( 1 - e^{-Z_{at}} \right)\right) + n_{at}, \quad \sigma_c^2 \Bigg) \\
\intertext{and}
 s_{at} &\sim N\Bigg( q_{a} + n_{at}, \quad \sigma_s^2 \Bigg)
\end{align*}
where in these equations we have written logs in lower case i.e. $c = \log C$.


\section{Stock assessment model details}

Modelled catches $C$ are defined in terms of the three quantities, natural mortality $M$, fishing mortality $F$ and recruitment $R$, using a modified form of the well known Baranov catch equation:

\begin{equation*}
  C_{ay} = \frac{\bm{F}_{ay}}{\bm{F}_{ay}+M_{ay}}\left(1 - e^{-(\bm{F}_{ay}+M_{ay})}\right) \bm{R}_{y}e^{-\sum (\bm{F}_{ay} + M_{ay})}
\end{equation*}

where $a$ and $y$ denote age and year. Modelled survey indices $I$ are defined in terms of the same three quantities with the addition of survey catchability $Q$:

\begin{equation*}
  I_{ays} = \bm{Q}_{ays} \bm{R}_{y}e^{-\sum (\bm{F}_{ay} + M_{ay})}
\end{equation*}

where $s$ denotes survey or abundance index and allows for multiple surveys to be considered. Observed catches $C^{(obs)}$ and the observed survey indices $I^{(obs)}$ are assumed to be log-normally distributed, or equivalently, normally distributed on the log-scale, with age, year and survey specific observation variance:

\begin{equation*}
  \log C^{(obs)}_{ay} \sim \text{Normal} \Big( \log C_{ay}, \bm{\sigma}^2_{ay}\Big) \qquad
  \log I^{(obs)}_{ays} \sim \text{Normal} \Big( \log I_{ays}, \bm{\tau}^2_{ays} \Big)
\end{equation*}

The full log-likelihood for the \aFa statistical catch at age model can now be defined as the sum of the log-likelihood of the observed catches ($\ell_N$ is the log-likelihood of a normal distribution)

\begin{equation*}
  \ell_C = \sum_{ay} w^{(c)}_{ay}\ \ell_N \Big( \log C_{ay}, \bm{\sigma}^2_{ay} ;\ \log C^{(obs)}_{ay} \Big)
\end{equation*}

and the log-likelihood of the observed survey indices

\begin{equation*}
  \ell_I = \sum_s \sum_{ay} w^{(s)}_{ays}\ \ell_N \Big( \log I_{ays}, \bm{\tau}_{ays}^2 ;\ \log I^{(obs)}_{ays} \Big)
\end{equation*}

giving the total log-likelihood

\begin{equation*}
  \ell = \ell_C + \ell_I
\end{equation*}

which is defined in terms of the strictly positive quantites, $M_{ay}$, $F_{ay}$, $Q_{ays}$ and $R_{y}$, and the observation variances $\sigma_{ay}$ and $\tau_{ays}$.  As such, the log-likelihood is over-parameterised as there are many more parameters than observations.  In order to reduce the number of parameters, $M_{ay}$ is assumed known (as is common), and the remaining parameters are written in terms of a linear combination of covariates $x_{ayk}$, e.g.

\begin{equation*}
  \log F_{ay} = \sum_k \beta_k x_{ayk}
\end{equation*}

where $k$ is the number of parameters to be estimated and is sufficiently small. Using this tecnique the quantities $\log F$, $\log Q$, $\log \sigma$ and $\log \tau$
%$\log \text{initial\,age\,structure}$ % this is not present in the above
(in bold in the equations above) can be described by a reduced number of parameters.  The following section has more discussion on the use of linear models in \aFa.


\subsubsection*{Stock recruitment relationships}

The \aFa statistical catch at age model can addiionally allow for a functional relationship to be imposed that links predicted recruitment $\tilde{R}$ based on spawning stock biomass and modelled recruitment $R$, included as a fixed variance random effect.  Options for the relationship are the hard coded models Ricker, Beverton Holt, smooth hockeystick or geometric mean. This is implemented by including a third component in the log-likelihood

\begin{equation*}
  \ell_{SR} = \sum_y \ell_N \Big( \log \tilde{R}_y(a, b), \phi_y^2 ;\ \log R_y \Big)
\end{equation*}

giving the total log-likelihood

\begin{equation*}
  \ell = \ell_C + \ell_I + \ell_{SR}
\end{equation*}

Using the (time varying) Ricker model as an example, predicted recruitment is

\begin{equation*}
  \tilde{R}_y(a_y,b_y) = a_y S_{y-1} e^{-b_y S_{y-1}}
\end{equation*}

where $S$ is spawning stock biomass derived from the model parameters $F$ and $R$, and the fixed quantites $M$ and mean weights by year and age. It is assumed that $R$ is log-normally distributed, or equivalently, normally distributed on the log-scale about the (log) recruitment predicted by the SR model $\tilde{R}$, with known variance $\phi^2$, i.e.

\begin{equation*}
  \log R_y \sim \text{Normal} \Big( \log \tilde{R}_y, \phi_y^2 \Big)
\end{equation*}

which leads to the definition of $\ell_{SR}$ given above. In all cases $a$ and $b$ are strictly positive, and with the quantities $F$, $R$, etc. linear models are used to parameterise $\log a$ and/or $\log b$, where relevant.

By default, recruitment $R$ as apposed to the reruitment predicted from a stock recruitment model $\tilde{R}$, is specified as a linear model with a parameter for each year, i.e.

\begin{equation*}
  \log R_y = \gamma_y
\end{equation*}

This is to allow modelled recruitment $R_y$ to be shrunk towards the stock recruitment model.  However, if it is considered appropriate that recruitment can be determined exactly by a relationship with covariates, it is possible, to instead define $\log R$ in terms of a linear model in the same way as $\log F$, $\log Q$, $\log \sigma$ and $\log \tau$.  %But this is pretty much the same as taking a geometric mean, with a model on log a, and making the variance very small.
















\subsection{Model fitting}

The parameters that are estimated are log recruitment $r_t$, survey catchability $q_a$, the $F$ parameters (MORE ON THESE LATER) and log stock numbers $n_{a1}$ in the first year.  The model is written in terms of these parameters,
\begin{align*}
 c_{at} &= r_{t-a+1} -\sum_{i=1}^{a-1} Z_{a-i, t-i} + \log \left( \frac{F_{at}}{Z_{at}} \left( 1 - e^{-Z_{at}} \right)\right) + \epsilon_{at} \\
 s_{at} &=  q_{a} + r_{t-a+1} -\sum_{i=1}^{a-1} Z_{a-i, t-i} + \epsilon'_{at}
\end{align*}
where $\epsilon$ denotes the Gaussian observation error.  There are some modifications required for the early cohorts as they use $n_{a1}$ rather than recruits and for the plus groups, but these are trivial and not presented.  It is possible to write these equations in matrix notation if we combine all catches into a single vector: $\bm{c} = (c_{11}, c_{21}, \ldots, c_{A_+1}, c_{12}, \ldots, c_{A_+2}, \ldots)^T$ and do similarly for the survey indices $\bm{s}$, it is also simpler if we define $\bm{r} = (n_{A1},n_{A-1,1},\ldots,n_{21},r_1,\ldots)^T$ and combine the F model parameters into a single vector $\bm{f}$ then we can write the model as
\begin{align*}
 \bm{c} &= \bm{X}_r \bm{r} + o_1(\bm{f}) + o_2(\bm{f}) + \bm{\epsilon} \\
 \bm{s} &=  \bm{M}\bm{X}_q \bm{q} + \bm{M}\bm{X}_r \bm{r} + \bm{M} o_1(\bm{f}) + \bm{\epsilon}'
\end{align*}
where the functions $o_1$ and $o_2$ are nonlinear (vector) functions of the $F$ model parameters and the $\bm{X}$ matrices are various design matrices based on the full set of ages and years (to be described later). The $\bm{M}$ matrix maps the correct age and year in the survey to that in the full set of ages and years.  These equations can be further combined by stacking the equations
\begin{align*}
 \begin{pmatrix} \bm{c} \\ \bm{s} \end{pmatrix}
   &= \begin{pmatrix} \bm{0} & \bm{X}_r \\ \bm{MX}_q & \bm{MX}_r \end{pmatrix}
      \begin{pmatrix} \bm{q} \\ \bm{r} \end{pmatrix} +
      \begin{pmatrix} \bm{I} & \bm{I} \\ \bm{M} & \bm{0} \end{pmatrix}
      \begin{pmatrix} o_1(\bm{f}) \\ o_2(\bm{f}) \end{pmatrix} +
      \begin{pmatrix} \bm{\epsilon} \\ \bm{\epsilon}' \end{pmatrix}
\end{align*}
so that the model is of the form
\begin{align*}
 \bm{y} = \bm{X}\bm{\beta} + o(\bm{f}) + \bm{\epsilon}
\end{align*}
where
\begin{align*}
 \bm{\epsilon} \sim N \Bigg( \bm{0}, \bm{W}^{-1} \Bigg)
   \quad \text{where} \quad
     \bm{W}^{-1} = \begin{pmatrix} \sigma_c^2\bm{I} & \bm{0} \\
                              \bm{0}  & \sigma_s^2\bm{I} \end{pmatrix}
                 = \sigma_c^2 \begin{pmatrix} \bm{I} & \bm{0} \\
                              \bm{0}  & \dfrac{\sigma_s^2}{\sigma_c^2}\bm{I} \end{pmatrix}
\end{align*}

In other words, this statistical catch at age model can be written as a linear model with an offset due to nonlinear functions of the F model parameters.  We use this to reduce the parameters in the fitting process by concentrating the likelihood.  This can be done by inserting the maximum likelihood estimates of $\beta$ conditional on $\bm{f}$ and $\bm{W}$ into the likelihood. The maximum likelihood estimate of $\beta$ conditional on the other parameters is
\begin{align*}
\hat{\bm{\beta}} = (\bm{X}^T\bm{WX})^{-1}\bm{X}^T\bm{W}(\bm{y} - o(\bm{f}))
\end{align*}
If we decide that the surveys and catches have the same observation variance then we can also estimate the obervation variance conditionally, in this case
\begin{align*}
  \hat{\bm{\beta}}(\bm{f}) &= (\bm{X}^T\bm{X})^{-1}\bm{X}^T(\bm{y} - o(\bm{f})) \\
  \hat{\bm{y}}(\bm{f}) &= \bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T(\bm{y} - o(\bm{f})) + o(\bm{f}) \\
                       &= \bm{H}(\bm{y} - o(\bm{f})) + o(\bm{f}) \\
                       &= \bm{H}\bm{y} + (1-\bm{H})o(\bm{f}) \\
  \intertext{and the conditional (unbiased) estimate of $\sigma^2$ is}
  \hat{\sigma^2}(\bm{f}) &= \frac{1}{n-p}\log\Big((\hat{\bm{y}}(\bm{f}) - \bm{y})^T(\hat{\bm{y}}(\bm{f}) - \bm{y})\Big)
\end{align*}
where $n$ is the length of $\bm{y}$ and $p$ is the number of unique parameters in $\beta$. The concentrated log likelihood for $\bm{f}$ is then
\begin{align*}
  l(\bm{f}) &\propto -\frac{n}{2}\log\Big((\hat{\bm{y}}(\bm{f}) - \bm{y})^T(\hat{\bm{y}}(\bm{f}) - \bm{y})\Big) \\
            &= -\frac{n}{2}\log\Bigg(\Big|\Big| (1-\bm{H})\bm{y} + (1-\bm{H})o(\bm{f})\Big|\Big|\Bigg)
\end{align*}
Since $1-\bm{H}$ and $(1-\bm{H})\bm{y}$ only depend on the data these can be calculated outside of an iterative optimization procedure.  It is straightforward to give different weights to the survey and catch components without increased computation (this is the same as assuming you know the ratio $\frac{\sigma_c^2}{\sigma_s^2}$).  However, if you want to estimate both variances then the estimate of $\beta$ is
\begin{align*}
  \hat{\bm{\beta}}(\bm{f}) &= (\bm{X}^T\bm{WX})^{-1}\bm{X}^T\bm{W}(\bm{y} - o(\bm{f})) \\
  \hat{\bm{y}}(\bm{f}) &= \bm{X}(\bm{X}^T\bm{WX})^{-1}\bm{X}^T\bm{W}(\bm{y} - o(\bm{f})) + o(\bm{f})
\end{align*}
and as $\bm{W}$ is not known before hand the inverse $(\bm{X}^T\bm{WX})^{-1}$ must be computed at every iteration.  The concentrated log likelihood in this case is
\begin{align*}
  l(\bm{f}) &= \frac{1}{2}\log|\bm{W}| -\frac{1}{2} \Big((\hat{\bm{y}}(\bm{f}) - \bm{y})^T\bm{W}(\hat{\bm{y}}(\bm{f}) - \bm{y})\Big) \\
            &= -\frac{n_c}{2}\log\sigma_c^2 - \frac{n_s}{2}\log\sigma_s^2 -\frac{1}{2} \Big((\hat{\bm{y}}(\bm{f}) - \bm{y})^T\bm{W}(\hat{\bm{y}}(\bm{f}) - \bm{y})\Big)
\end{align*}


\section{Extending the model (brief)}

First potential models for $q_a$.  Linear forms can be included my simply changing the design matrices.  By linear forms i mean spline smoothers with fixed degrees of freedom.  An interesting addition would be to use penalised splines or better (i think) structured random effects \citep[GMRFs i.e. 1st order and 2nd order random walks, see for example][]{Rue_Held.2005}.  Structured random effect models using GMRFs require a Bayesian approach.  If the variance of these is assumed known then these random effects can be integrated out directly, but if the variance is to be estimated it must be done in the outside iteration along with the observation error and the F parameters.

The model for F has not been considered so far.  Options for this are a simple separable model, a model with several separable periods all these models can be expressed as linear models on the log link.  Within the same framework is simple then to include smoothers (splines) with fixed degrees of freedom.  More interesting but perhaps out with the scope of this project are structured random effect models for F.  These include seasonal models (treating the number of ages as the season length) and correlated random walks.

\section{Summary}

At its simplest the model is a non-linear fixed effects regression, fitting the F parameters to the data but this requires that the survey variances are known relative to the catch variance.  This model can include splines, or random effects with known parameters (variances, degrees of freedom, autocorrelation) in the q and r models.  The reason this is being considered is that computational speed is important and allowing some fixed variability may reduce bias in model outputs by allowing some flexibility, it is acknowledged that such assumptions make statistical testing a bit dodgy.

The next level of complexity is where we want to estimate the survey variance.  This means we have to recalculate the hat matrix at every iteration which involves a matrix inversion.  Since the matrix inversion is being done already, including structured random effects for q and recruitment into this only adds the variance parameters to the objective function.

If a stock recruit function were to be added, recruits could not be integrated out and would have to be estimated in the objective function.

If more complicated F models were used, such as structured random effects (random walks, correlated random walks, seasonal models) the number of parameters would increase in the objective function.  Therefore a model with few parameters is one with a highly parametrised F model and no SRR relationship.

\section{Implementation (sketch)}

We require two functions that return an objective
\begin{enumerate}
  \item[A] inputs are F at age and observation error and arguments are the data and the hat matrix
  \item[B] inputs are F at age  and any variance parameters taking as arguments the design matrix $H$, the weight matrix $W$ and the structural prior matrix (not mentioned yet but lets call it $Q$)
\end{enumerate}

The full objective function is then
\begin{enumerate}
  \item take input parameters (F pars, variances, recruitments (if SRR model being used))
  \item convert F pars into F at age
  \item calculate objective value using one of the two functions A or B above
  \item add on SRR density and prior densities for variances if necessary
\end{enumerate}









\iffalse % ignoring this bit


\begin{align}
  N_{at} &=
  \left\lbrace
    \begin{array}{ll}
      R_t & \text{if} \quad a = 1 \\
      R_{t-a+1} e^{ -\sum_{i=1}^{a-1} Z_{a-i,t-i}} & \text{if} \quad a > 1
    \end{array} \right.
\end{align}


\textcolor{red}{EJ: We need to separate the model(s) from their fitting. I'd like to be more "fisheries science"-like in the general description. We need to work on it, but I like the idea that we have a single model based on the common dynamics and we increase the complexity by adding S/R, Selectivity or growth models, depending on the information available. That means using the usual equations for the dynamics. Afterwards we can add a section on fitting which shows all the technical details, including how it links with the model.}

The data are assumed to be gaussian observations of numbers at age in the population, which itself is generated by the common age structured model

\begin{align}
  n_{at} &=
  \left\lbrace
    \begin{array}{ll}
      r_t & \text{if} \quad a = 1 \\
      r_t -\sum_{i=1}^{a-1} Z_{a-i,t-i} & \text{if} \quad a > 1
    \end{array} \right.
\end{align}

where $Z_{at} = F_{at} + M_{at}$, where $M_{at}$ is known and $F_{at}$ is modelled as a seperable function
\begin{align}
  \log F_{at} = \gamma_a + \delta_t
\end{align}
with suitable constraints.

The observation equations are

\begin{align}
 c_{at} &\sim N \Bigg( \log \left( \frac{F_{at}}{Z_{at}} \left( 1 - e^{-Z_{at}} \right)\right) + n_{at}, \quad \kappa \Bigg) \\
\intertext{and}
 s_{atk} &\sim N\Bigg( q_{ak} + n_{at}, \quad \tau_k \Bigg)
\end{align}


The parameters to be estimated in this model are: log recruitment $r_t$, F at age and year, $F_{at}$, log survey catchability $q_{ak}$, and the precisions $\theta = (\kappa, \tau_1, \ldots)$.

\section{model details / parameterisations}

\subsection{Almost linear models}

The approach taken here is built around the observation that the model can be written in the form

\begin{align}
  \mathbf{y} = f(\alpha) + X\beta + \epsilon
\end{align}

So that conditional on $\alpha$, the model is linear with gaussian errors, so there are nice properties to be exploited.  The question then is, how much can we squeeze into $\beta$...

If the survey catchability model is linear, the parameters $\alpha$ define $F_{at}$ and the function $f$ is a non-linear function involving $F$ and $M$. So that recruitment and survey catchabilities are contained in $\beta$. A full description of this definition of $f$ and $X$ is given in appendix \ref{sec:designmats}.  This is in a sense the best we can do, as there is no way to (analytically) linearise the model with respect to $F_{at}$.

Two ways in which structure can be introduced to this model is through parametric forms, for example using a basis smoother $q_{ak} = B_q\beta_{q}$ or through structural priors, for example by imposing that
\begin{align*}
  (q_{2k} - q_{1k}) - (q_{3k} - q_{2k}) \sim N\bigg(0, \lambda \bigg)
\end{align*}
That is deviations from a straight line are penalised much like a penalized smoother (see Rue and Held for details).  Other priors based on the normal distribution that could be of use are things like:  smooth shapes for $q$, smooth shapes with flat tops or with plataues defined in terms of restrictions to gradients or step sizes, random walk or AR1 process (plus optional smooth trend) for recruitment, an even an evolving pattern for $q$ or $F$ over time (\emph{but this might be pushing it a bit!}). See section \ref{sec:modelshapes} for a description of these priors.

Since $F_{at}$ is always non-linear there is no reason to prefer structural priors over parameteric forms (such as logistic, double normal etc.) unless one wishes to take advantage of the flexiblilty of the structural prior models.

\subsection{Not so linear models}

Aside from nice linear priors, there is also the possibility of a recruitment model being imposed.  These are of the form
\begin{align}
  E \big[ r_{t+1} \big] = f\left( \sum_a \omega_{at} e^{n_{at}} \right)
\end{align}
and so in general if there is a recruitment model specified the only parameters that are linear are the survey catchabilities.

\subsection{Totally non-linear models}

Finaly if there is a non-linear function specified for the survey catchabilities the model is completely non-linear.

\section{Fitting strategies}

\subsection{Almost linear models}

In this case the model, in full generality, is
\begin{align*}
  \mathbf{y} &= f(\alpha) + X\beta + \epsilon \\
\intertext{where}
  \alpha &\sim N\big(0,Q_a(\theta)\big) \\
  \beta &\sim N\big(0,Q_b(\theta)\big) \\
  \epsilon &\sim N\big(0,H(\theta)\big) \\
\intertext{and}
  \theta &\sim \text{uninformatively}
\end{align*}
where the $Q$ matrices are structural priors, the $H$ matrix is diagonal with different variances for each component and all the variance parameters are contained in $\theta$ which are given uninformative priors.  For precisions these will be gamma priors and for AR1 parameters these will be ...

As all the priors are gaussian, we can integrate out $\beta$ leaving us with
\begin{align*}
  \log \pi(\alpha, \theta | y) \propto
      -\frac{1}{2} \Big[ &(y - f(\alpha))^T ( H - HX(X^THX + Q_b)^{-1}X^TH) (y - f(\alpha)) \\
      &+ \alpha^T Q_a \alpha - \log \frac{|Q_a||Q_b|}{|X^THX+Q_b|} \Big] + \log \pi(\theta)
\end{align*}
the joint distribution of $\alpha$ and $\theta$ to maximise. Note that the matrices $Q_a$, $Q_b$ and $H$ all depend on $\theta$.

\textbf{\emph{An additional possibility...}}

\emph{Conditional on $\alpha$ (and the components of $\theta$ that define $Q_a$), the joint distribution above becomes a kind of multivariate chi-square distribution, and i suspect this has an analytically tractable maximum, so that we could plug in the best estimates of the remaining $\theta$...}

The strategy therefore is to maximise over $\alpha$ and $\theta$.  If we can plug in values of $\theta$ that maximise the distribution $\pi(\theta|\alpha,y)$, then we could propose $\alpha$, plug in best $\theta$, $\tilde{\theta}$ say, and return the value of the joint density at the point $(\alpha,\tilde{\theta})$.  Proposals would be calculated by the optimiser - nlminb for example.


\subsection{Not so linear models}

These models can be treated the same as in the previous section, however the size of $\alpha$ is now much larger as it contains all the recruitments $r_t$ and only the catchabilities remain in the $\beta$ vector.

\subsection{Totally non-linear models}

With these models we are resigned to maximising $\log \pi(\alpha, \beta, \theta | y)$.  I am not aware of any tricks here.


\section{The structure in the model\\ {\small or} \\ What the parameters will look like}
\label{sec:modelshapes}

\subsection{log Catchabilities: $q_a$ (and possibly F at age)}

A simple non parametric model is an approximation to a thin plate spline with 3 or 4 degrees of freedom.  The problem with these smoothers is that the smaller the degrees of freedom the poorer the approximation - these smoothers usually perform better in a penalised setting where the model space contains more degrees of freedom than required.  This problem can be cast into a random effects setting, but it is simpler to set a prior that penalizes deviations form a straight line (this is pretty much the discrete analogue of the thin plate spline anyway).  Such a prior looks like this:
 \begin{align*}
  (x_{i-1} - x_{i-2}) - (x_i - x_{i-1}) &\sim N\bigg(0, \lambda \bigg)
\intertext{or}
  Dx &\sim N\bigg(0, \lambda I \bigg)
\end{align*}
in other words $x$ is gaussian with zero mean and precision matrix $Q = D^TD$.  This also gives an insight into why this prior is a smoother.  The second difference matrix $D$ enters into the posterior as $\lambda x^TD^TDx$ i.e. the sum of the squared (discrete) second derivatives of $x$ and weighted by the variance of the gradient changes - this is analagous to the smoothing penalties, typically written $\lambda \int f''(x)^2$.

Related priors are the random walk defined by
\begin{align*}
  x_i - x_{i-1} &\sim N\bigg(0, \lambda \bigg)
\end{align*}
Combinations of these priors can be used, in particular to impose a flat (or flatish) curve beyond a given point, one can specify a weight, $w_i$ on the variances so that the differences are forced to be effectively zero.  In adition to this you could also make the curve smoother as this point is approached forcing somthing like a flat top.  The details of this have still to be finalised.

%A point to note about forcing something like a flat top when the data strongly suggests a dome is that if you believe that the data produce a dome due to missing ages in the catch for example, then forcing a flat top will not remove this bias.



\begin{figure}[H]
\centering
%<<fig=TRUE, echo=FALSE, width=6, height=4>>=
%a <- seq(1,9,length=100)
%X <- model.matrix(gam(x ~ s(a, k = 4), data = list(x = rep(1,length(a)), a = a)))
%plot(a, a, ylim = range(X), ylab = "", xlab = "Age", type = "n", las = 1)
%tmp <- apply(X, 2, function(y) lines(a, y))
%@
\caption{Basis functions for a thin plate spline}
\label{fig:tpbfuns}
\end{figure}

\begin{figure}[H]
\centering
%<<fig=TRUE, echo=FALSE, width=6, height=4>>=
%a <- seq(1,9,length=100)
%x <- matrix(c(-1.2,0,0,-1, -.6,-1,0,0, -2,0,8,0, -.5,1,1,0), nrow = 4)
%y <- exp(X %*% x)
%plot(a, y[,1], ylim = c(0,max(y)), ylab = "", xlab = "Age", type = "n", las = 1)
%tmp <- apply(y, 2, function(y) lines(a, y))
%@
\caption{Some selectivity functions}
\label{fig:selfuns}
\end{figure}


\subsection{Recruitment models}



\appendix

\section{Design matrices}
\label{sec:designmats}

Not quite yet

\fi


\iffalse % ignoring this bit

\section{Model derivatives}

move these to an appendix if we want to keep them.

let
\begin{align}
  l(\btheta) = -\frac{1}{2\sigma^2} \sum_{ay} (x_{ay} - f(\theta;a,y))^2
\end{align}
then
\begin{align}
  \pdx{l}{\theta} = \frac{1}{2\sigma^2} \sum_{ay} 2(f(\theta;a,y) - x_{ay}) \pdx{f(\theta;a,y)}{\theta}
\end{align}
So all we need are the derivatives of the mean function $f$.  Recalling that
\begin{align}
  \log N(\theta;a,y) &=
  \left\lbrace
    \begin{array}{ll}
      r_y & \text{if} \quad a = 1 \\
      r_y -\sum_{i=1}^{a-1} \left( S(\balpha;a-i)F_{y-i} + M_{a-i,y-i} \right) & \text{if} \quad a > 1
    \end{array} \right. \\
 \log C(\theta;a, y) &= \log S(\balpha;a) + \log F_y - \log(S(\balpha;a) F_y + M_{ay}) \\
 &\qquad \qquad + \log \left( 1 - e^{-S(\balpha;a)F_y - M_{ay}} \right)
           + \log N(\theta;a,y) \\
  \log I(\theta;a,y) &= \log q(\bbeta;a) + \log N(\theta;a,y)
\end{align}

Then for $f = \log N$
\begin{align}
  \pdx{}{r_j}f(\theta;a,y) &= \left\lbrace \begin{array}{ll} 0 & \text{if} \quad j \neq y \\ 1 & \text{if} \quad j = y \end{array} \right. \\
  \pdx{}{\balpha}f(\theta;a,y) &= \sum_i^{a-1} F_{y-a+i} \pdx{}{\balpha}S(\balpha;i) \\
  \pdx{}{\bbeta}f(\theta;a,y) &= 0 \\
  \pdx{}{F_j}f(\theta;a,y) &= \left\lbrace \begin{array}{lc} s(a-y+j) & \text{if} \quad a>1 \text{ and } y \geq a \\ 0 & \text{otherwise} \end{array} \right. \\
\intertext{For $f = \log I$, $\pdx{}{\theta}f = \pdx{}{\theta} \log N$ except for}
  \pdx{}{\bbeta}f(\theta;a,y) &= \frac{1}{q(\bbeta;a)} \pdx{}{\bbeta} q(\bbeta;a) \\
\intertext{And for $f = \log C$, $\pdx{}{\theta}f = \pdx{}{\theta} \log N$ except for}
  \pdx{}{\balpha}f(\theta;a,y) &= \text{otherstuff } + \sum_i^{a-1} F_{y-a+i} \pdx{}{\balpha}S(\balpha;i) \\
  \pdx{}{F_j}f(\theta;a,y) &= \text{otherstuff } + \left\lbrace \begin{array}{lc} s(a-y+j) & \text{if} \quad a>1 \text{ and } y \geq a \\ 0 & \text{otherwise} \end{array} \right. \\
\end{align}

\fi


\bibliographystyle{chicago}
\bibliography{a4a_refs}


\end{document}
